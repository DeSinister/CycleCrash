{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoTransformer\n",
    "\n",
    "TimeSformer(https://arxiv.org/abs/2102.05095), ViViT(https://arxiv.org/abs/2103.15691)\n",
    "\n",
    "Welcome to the demo notebook for VideoTransformer. We'll showcase the prediction result by the above pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "This section contains initial setup. Run it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gv1qdarnNNVE",
    "outputId": "eaac44da-a976-4fda-b323-03dfa6790e21",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "!pip3 install --user torch\n",
    "!pip3 install --user torchvision\n",
    "!pip3 install --user matplotlib\n",
    "!pip3 install --user decord\n",
    "!pip3 install --user einops\n",
    "!pip3 install --user scikit-image\n",
    "!pip3 install --user pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBBG_T32pzPH",
    "outputId": "76921b65-9e35-4260-fa0a-bbb484ac285c",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from IPython.display import display\n",
    "\n",
    "!git clone https://github.com/mx-mark/VideoTransformer-pytorch.git\n",
    "%cd VideoTransformer-pytorch\n",
    "\n",
    "import data_transform as T\n",
    "from dataset import DecordInit, load_annotation_data\n",
    "from transformer import PatchEmbed, TransformerContainer, ClassificationHead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Please firstly dowload the weights and move to the current path `./VideoTransformer-pytorch/`\n",
    "1. TimeSformer-B pre-trained on K400 https://drive.google.com/file/d/1jLkS24jkpmakPi3e5J8KH3FOPv370zvo/view?usp=sharing\n",
    "2. ViViT-B pre-trained on K400 from https://drive.google.com/file/d/1-JVhSN3QHKUOLkXLWXWn5drdvKn0gPll/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yLko87R2_m4",
    "pycharm": {}
   },
   "source": [
    "## Video Transformer Model\n",
    "\n",
    "We here load the pretrained weights of the transformer model TimeSformer-B or ViViT-B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSformer(nn.Module):\n",
    "    \"\"\"TimeSformer. A PyTorch impl of `Is Space-Time Attention All You Need for\n",
    "    Video Understanding? <https://arxiv.org/abs/2102.05095>`_\n",
    "\n",
    "    Args:\n",
    "        num_frames (int): Number of frames in the video.\n",
    "        img_size (int | tuple): Size of input image.\n",
    "        patch_size (int): Size of one patch.\n",
    "        pretrained (str | None): Name of pretrained model. Default: None.\n",
    "        embed_dims (int): Dimensions of embedding. Defaults to 768.\n",
    "        num_heads (int): Number of parallel attention heads in\n",
    "            TransformerCoder. Defaults to 12.\n",
    "        num_transformer_layers (int): Number of transformer layers. Defaults to\n",
    "            12.\n",
    "        in_channels (int): Channel num of input features. Defaults to 3.\n",
    "        dropout_p (float): Probability of dropout layer. Defaults to 0.\n",
    "        conv_type (str): Type of the convolution in PatchEmbed layer. Defaults to Conv2d.\n",
    "        attention_type (str): Type of attentions in TransformerCoder. Choices\n",
    "            are 'divided_space_time', 'space_only' and 'joint_space_time'.\n",
    "            Defaults to 'divided_space_time'.\n",
    "        norm_layer (dict): Config for norm layers. Defaults to nn.LayerNorm.\n",
    "        return_cls_token (bool): Whether to use cls_token to predict class label.\n",
    "    \"\"\"\n",
    "    supported_attention_types = [\n",
    "        'divided_space_time', 'space_only', 'joint_space_time'\n",
    "    ]\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_frames,\n",
    "                 img_size,\n",
    "                 patch_size,\n",
    "                 embed_dims=768,\n",
    "                 num_heads=12,\n",
    "                 num_transformer_layers=12,\n",
    "                 in_channels=3,\n",
    "                 conv_type='Conv2d',\n",
    "                 dropout_p=0.,\n",
    "                 attention_type='divided_space_time',\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 return_cls_token=True,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        assert attention_type in self.supported_attention_types, (\n",
    "            f'Unsupported Attention Type {attention_type}!')\n",
    "\n",
    "        self.num_frames = num_frames\n",
    "        self.embed_dims = embed_dims\n",
    "        self.num_transformer_layers = num_transformer_layers\n",
    "        self.attention_type = attention_type\n",
    "        self.conv_type = conv_type\n",
    "        self.return_cls_token = return_cls_token\n",
    "\n",
    "        #tokenize & position embedding\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dims=embed_dims,\n",
    "            conv_type=conv_type)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # Divided Space Time Attention\n",
    "        operator_order = ['time_attn','space_attn','ffn']\n",
    "        container = TransformerContainer(\n",
    "            num_transformer_layers=num_transformer_layers,\n",
    "            embed_dims=embed_dims,\n",
    "            num_heads=num_heads,\n",
    "            num_frames=num_frames,\n",
    "            norm_layer=norm_layer,\n",
    "            hidden_channels=embed_dims*4,\n",
    "            operator_order=operator_order)\n",
    "\n",
    "        self.transformer_layers = container\n",
    "        self.norm = norm_layer(embed_dims, eps=1e-6)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dims))\n",
    "        num_patches = num_patches + 1\n",
    "\n",
    "        # spatial pos_emb\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,num_patches,embed_dims))\n",
    "        self.drop_after_pos = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        # temporal pos_emb\n",
    "        self.time_embed = nn.Parameter(torch.zeros(1,num_frames,embed_dims))\n",
    "        self.drop_after_time = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size[0]\n",
    "        h0 = h // self.patch_embed.patch_size[0]\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Tokenize\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Add Position Embedding\n",
    "        cls_tokens = repeat(self.cls_token, 'b ... -> (repeat b) ...', repeat=x.shape[0])\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h) #self.pos_embed\n",
    "        x = self.drop_after_pos(x)\n",
    "\n",
    "        # Add Time Embedding\n",
    "        cls_tokens = x[:b, 0, :].unsqueeze(1)\n",
    "        x = rearrange(x[:, 1:, :], '(b t) p d -> (b p) t d', b=b)\n",
    "        x = x + self.time_embed\n",
    "        x = rearrange(x, '(b p) t d -> b (p t) d', b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = self.drop_after_time(x)\n",
    "\n",
    "        # Video transformer forward\n",
    "        x = self.transformer_layers(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        # Return Class Token\n",
    "        if self.return_cls_token:\n",
    "            return x[:, 0]\n",
    "        else:\n",
    "            return x[:, 1:].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViViT(nn.Module):\n",
    "    \"\"\"ViViT. A PyTorch impl of `ViViT: A Video Vision Transformer`\n",
    "        <https://arxiv.org/abs/2103.15691>\n",
    "\n",
    "    Args:\n",
    "        num_frames (int): Number of frames in the video.\n",
    "        img_size (int | tuple): Size of input image.\n",
    "        patch_size (int): Size of one patch.\n",
    "        pretrained (str | None): Name of pretrained model. Default: None.\n",
    "        embed_dims (int): Dimensions of embedding. Defaults to 768.\n",
    "        num_heads (int): Number of parallel attention heads. Defaults to 12.\n",
    "        num_transformer_layers (int): Number of transformer layers. Defaults to 12.\n",
    "        in_channels (int): Channel num of input features. Defaults to 3.\n",
    "        dropout_p (float): Probability of dropout layer. Defaults to 0..\n",
    "        tube_size (int): Dimension of the kernel size in Conv3d. Defaults to 2.\n",
    "        conv_type (str): Type of the convolution in PatchEmbed layer. Defaults to Conv3d.\n",
    "        attention_type (str): Type of attentions in TransformerCoder. Choices\n",
    "            are 'divided_space_time', 'fact_encoder' and 'joint_space_time'.\n",
    "            Defaults to 'fact_encoder'.\n",
    "        norm_layer (dict): Config for norm layers. Defaults to nn.LayerNorm.\n",
    "        copy_strategy (str): Copy or Initial to zero towards the new additional layer.\n",
    "        extend_strategy (str): How to initialize the weights of Conv3d from pre-trained Conv2d.\n",
    "        use_learnable_pos_emb (bool): Whether to use learnable position embeddings.\n",
    "        return_cls_token (bool): Whether to use cls_token to predict class label.\n",
    "    \"\"\"\n",
    "    supported_attention_types = [\n",
    "        'fact_encoder', 'joint_space_time', 'divided_space_time'\n",
    "    ]\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_frames,\n",
    "                 img_size,\n",
    "                 patch_size,\n",
    "                 embed_dims=768,\n",
    "                 num_heads=12,\n",
    "                 num_transformer_layers=12,\n",
    "                 in_channels=3,\n",
    "                 dropout_p=0.,\n",
    "                 tube_size=2,\n",
    "                 conv_type='Conv3d',\n",
    "                 attention_type='fact_encoder',\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 return_cls_token=True,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        assert attention_type in self.supported_attention_types, (\n",
    "            f'Unsupported Attention Type {attention_type}!')\n",
    "\n",
    "        num_frames = num_frames//tube_size\n",
    "        self.num_frames = num_frames\n",
    "        self.embed_dims = embed_dims\n",
    "        self.num_transformer_layers = num_transformer_layers\n",
    "        self.attention_type = attention_type\n",
    "        self.conv_type = conv_type\n",
    "        self.tube_size = tube_size\n",
    "        self.num_time_transformer_layers = 4\n",
    "        self.return_cls_token = return_cls_token\n",
    "\n",
    "        #tokenize & position embedding\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dims=embed_dims,\n",
    "            tube_size=tube_size,\n",
    "            conv_type=conv_type)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # Divided Space Time Transformer Encoder - Model 2\n",
    "        transformer_layers = nn.ModuleList([])\n",
    "\n",
    "        spatial_transformer = TransformerContainer(\n",
    "            num_transformer_layers=num_transformer_layers,\n",
    "            embed_dims=embed_dims,\n",
    "            num_heads=num_heads,\n",
    "            num_frames=num_frames,\n",
    "            norm_layer=norm_layer,\n",
    "            hidden_channels=embed_dims*4,\n",
    "            operator_order=['self_attn','ffn'])\n",
    "\n",
    "        temporal_transformer = TransformerContainer(\n",
    "            num_transformer_layers=self.num_time_transformer_layers,\n",
    "            embed_dims=embed_dims,\n",
    "            num_heads=num_heads,\n",
    "            num_frames=num_frames,\n",
    "            norm_layer=norm_layer,\n",
    "            hidden_channels=embed_dims*4,\n",
    "            operator_order=['self_attn','ffn'])\n",
    "\n",
    "        transformer_layers.append(spatial_transformer)\n",
    "        transformer_layers.append(temporal_transformer)\n",
    "\n",
    "        self.transformer_layers = transformer_layers\n",
    "        self.norm = norm_layer(embed_dims, eps=1e-6)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dims))\n",
    "        # whether to add one cls_token in temporal pos_enb\n",
    "        num_frames = num_frames + 1\n",
    "        num_patches = num_patches + 1\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,num_patches,embed_dims))\n",
    "        self.time_embed = nn.Parameter(torch.zeros(1,num_frames,embed_dims))\n",
    "        self.drop_after_pos = nn.Dropout(p=dropout_p)\n",
    "        self.drop_after_time = nn.Dropout(p=dropout_p)\n",
    "    \n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size[0]\n",
    "        h0 = h // self.patch_embed.patch_size[0]\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Tokenize\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Add Position Embedding\n",
    "        cls_tokens = repeat(self.cls_token, 'b ... -> (repeat b) ...', repeat=x.shape[0])\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "        x = self.drop_after_pos(x)\n",
    "\n",
    "        # fact encoder - CRNN style\n",
    "        spatial_transformer, temporal_transformer, = *self.transformer_layers,\n",
    "        x = spatial_transformer(x)\n",
    "\n",
    "        # Add Time Embedding\n",
    "        cls_tokens = x[:b, 0, :].unsqueeze(1)\n",
    "        x = rearrange(x[:, 1:, :], '(b t) p d -> b t p d', b=b)\n",
    "        x = reduce(x, 'b t p d -> b t d', 'mean')\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.time_embed\n",
    "        x = self.drop_after_time(x)\n",
    "\n",
    "        x = temporal_transformer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        # Return Class Token\n",
    "        if self.return_cls_token:\n",
    "            return x[:, 0]\n",
    "        else:\n",
    "            return x[:, 1:].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_state_dict(state_dict):\n",
    "\tfor old_key in list(state_dict.keys()):\n",
    "\t\tif old_key.startswith('model'):\n",
    "\t\t\tnew_key = old_key[6:]\n",
    "\t\t\tstate_dict[new_key] = state_dict.pop(old_key)\n",
    "\t\telse:\n",
    "\t\t\tnew_key = old_key[9:]\n",
    "\t\t\tstate_dict[new_key] = state_dict.pop(old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_from_pretrain_(module, pretrained, init_module):\n",
    "    if torch.cuda.is_available():\n",
    "        state_dict = torch.load(pretrained)\n",
    "    else:\n",
    "        state_dict = torch.load(pretrained, map_location=torch.device('cpu'))\n",
    "    if init_module == 'transformer':\n",
    "        replace_state_dict(state_dict)\n",
    "    elif init_module == 'cls_head':\n",
    "        replace_state_dict(state_dict)\n",
    "    else:\n",
    "        raise TypeError(f'pretrained weights do not include the {init_module} module')\n",
    "    msg = module.load_state_dict(state_dict, strict=False)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "u5J7lGPJ2bLJ",
    "outputId": "41c0568b-082f-4609-8a5e-9ff4b80ffd92",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "num_frames = 8\n",
    "frame_interval = 32\n",
    "num_class = 400\n",
    "arch = 'timesformer' # turn to vivit for initializing vivit model\n",
    "\n",
    "if arch == 'timesformer':\n",
    "    pretrain_pth = './timesformer_k400.pth'\n",
    "    model = TimeSformer(num_frames=num_frames,\n",
    "                        img_size=224,\n",
    "                        patch_size=16,\n",
    "                        embed_dims=768,\n",
    "                        in_channels=3,\n",
    "                        attention_type='divided_space_time',\n",
    "                        return_cls_token=True)\n",
    "elif arch == 'vivit':\n",
    "    pretrain_pth = './vivit_model.pth'\n",
    "    num_frames = num_frames * 2\n",
    "    frame_interval = frame_interval // 2\n",
    "    model = ViViT(num_frames=num_frames,\n",
    "                  img_size=224,\n",
    "                  patch_size=16,\n",
    "                  embed_dims=768,\n",
    "                  in_channels=3,\n",
    "                  attention_type='fact_encoder',\n",
    "                  return_cls_token=True)\n",
    "else:\n",
    "    raise TypeError(f'not supported arch type {arch}, chosen in (timesformer, vivit)')\n",
    "\n",
    "cls_head = ClassificationHead(num_classes=num_class, in_channels=768)\n",
    "msg_trans = init_from_pretrain_(model, pretrain_pth, init_module='transformer')\n",
    "msg_cls = init_from_pretrain_(cls_head, pretrain_pth, init_module='cls_head')\n",
    "model.eval()\n",
    "cls_head.eval()\n",
    "model = model.to(device)\n",
    "cls_head = cls_head.to(device)\n",
    "print(f'load model finished, the missing key of transformer is:{msg_trans[0]}, cls is:{msg_cls[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess\n",
    "\n",
    "Here we show the video demo and transform the video input for the model processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "video_path = './demo/YABnJL_bDzw.mp4'\n",
    "html_str = '''\n",
    "<video controls width=\\\"480\\\" height=\\\"480\\\" src=\\\"{}\\\">animation</video>\n",
    "'''.format(video_path)\n",
    "display(HTML(html_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data preprocess\n",
    "mean, std = (0.45, 0.45, 0.45), (0.225, 0.225, 0.225)\n",
    "data_transform = T.Compose([\n",
    "        T.Resize(scale_range=(-1, 256)),\n",
    "        T.ThreeCrop(size=224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std)\n",
    "        ])\n",
    "temporal_sample = T.TemporalRandomCrop(num_frames*frame_interval)\n",
    "\n",
    "# Sampling video frames\n",
    "video_decoder = DecordInit()\n",
    "v_reader = video_decoder(video_path)\n",
    "total_frames = len(v_reader)\n",
    "start_frame_ind, end_frame_ind = temporal_sample(total_frames)\n",
    "if end_frame_ind-start_frame_ind < num_frames:\n",
    "    raise ValueError(f'the total frames of the video {video_path} is less than {num_frames}')\n",
    "frame_indice = np.linspace(0, end_frame_ind-start_frame_ind-1, num_frames, dtype=int)\n",
    "video = v_reader.get_batch(frame_indice).asnumpy()\n",
    "del v_reader\n",
    "\n",
    "video = torch.from_numpy(video).permute(0,3,1,2) # Video transform: T C H W\n",
    "data_transform.randomize_parameters()\n",
    "video = data_transform(video)\n",
    "video = video.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Classification\n",
    "\n",
    "Here we use the pre-trained video transformer to classify the input video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "with torch.no_grad():\n",
    "    logits = model(video)\n",
    "    output = cls_head(logits)\n",
    "    output = output.view(3, 400).mean(0)\n",
    "    cls_pred = output.argmax().item()\n",
    "\n",
    "class_map = './k400_classmap.json'\n",
    "class_map = load_annotation_data(class_map)\n",
    "for key, value in class_map.items():\n",
    "    if int(value) == int(cls_pred):\n",
    "        print(f'the shape of ouptut: {output.shape}, and the prediction is: {key}')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "iBOT_demo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10cb415f29954cb0a511e263fcb6c69f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecfd9523d50f4f7f87083460dc0e3dd7",
      "placeholder": "​",
      "style": "IPY_MODEL_6d68baa05ccc427890286e0e74452155",
      "value": " 327M/327M [00:09&lt;00:00, 36.6MB/s]"
     }
    },
    "3cb57e69cf054551b1978ddd6be3553b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5bbd072a4a424ef88d5354793942a84b",
      "placeholder": "​",
      "style": "IPY_MODEL_6668b553344743e2a1450251c5a237e2",
      "value": "100%"
     }
    },
    "557451188ee74f3698a8b358ee70d29d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af53b2f6e65b43fb8b3df5c656b40c92",
      "max": 343279349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9e962b8959ca4e90b496ab36d764f073",
      "value": 343279349
     }
    },
    "5bbd072a4a424ef88d5354793942a84b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6668b553344743e2a1450251c5a237e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d68baa05ccc427890286e0e74452155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8902dbcd84b4439a987e908be8c8e19e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3cb57e69cf054551b1978ddd6be3553b",
       "IPY_MODEL_557451188ee74f3698a8b358ee70d29d",
       "IPY_MODEL_10cb415f29954cb0a511e263fcb6c69f"
      ],
      "layout": "IPY_MODEL_95c73030d06d4d578252f47989c208fc"
     }
    },
    "95c73030d06d4d578252f47989c208fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e962b8959ca4e90b496ab36d764f073": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af53b2f6e65b43fb8b3df5c656b40c92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecfd9523d50f4f7f87083460dc0e3dd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
